{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Delta Lake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating schema employeeDB on catalog spark_catalog\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  7|\n",
      "|  5|\n",
      "|  4|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.errors import PySparkException\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "\n",
    "def get_schemas_from_catalog(catalog_source):\n",
    "    try:\n",
    "        df_schemas = spark.sql(f\"SHOW SCHEMAS IN {catalog_source}\")\n",
    "        lst_schemas = [s[0] for s in df_schemas.select(\"databaseName\").collect()]\n",
    "    except:\n",
    "        print(f\"Catalog {catalog_source} does not exist.\")\n",
    "        lst_schemas = []\n",
    "    return lst_schemas\n",
    "\n",
    "\n",
    "def get_tables_from_schema(catalog_source, schema_name):\n",
    "    try:\n",
    "        df_tables = spark.sql(f\"SHOW TABLES IN {catalog_source}.{schema_name}\")\n",
    "        lst_tables = [t[0] for t in df_tables.select(\"tableName\").collect()]\n",
    "    except:\n",
    "        print(f\"Schema {schema_name} does not exist in {catalog_source}.\")\n",
    "        lst_tables = []\n",
    "    return lst_tables\n",
    "\n",
    "\n",
    "def get_container_from_catalog(catalog_target):\n",
    "    try:\n",
    "        df = spark.sql(f\"DESCRIBE CATALOG EXTENDED {catalog_target}\")\n",
    "        storage_location = df.filter(col(\"info_name\") == \"Storage Root\").collect()[0][1]\n",
    "        container_target = storage_location.split(\"@\")[0].replace(\"abfss://\", \"\")\n",
    "    except PySparkException as ex:\n",
    "        if ex.getErrorClass() == \"NO_SUCH_CATALOG_EXCEPTION\":\n",
    "            container_target = f\"Catalog {catalog_target} does not exist.\"\n",
    "        else:\n",
    "            raise\n",
    "    return container_target\n",
    "\n",
    "\n",
    "def create_init_catalog(catalog):\n",
    "    print(f\"Creating  catalog {catalog}\")\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "\n",
    "\n",
    "def create_init_schema(catalog, schema_name):\n",
    "    print(f\"Creating schema {schema_name} on catalog {catalog}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}\")\n",
    "\n",
    "\n",
    "def create_schema(catalog_source, catalog_target, schema_name):\n",
    "    df_schemas = spark.sql(f\"SHOW SCHEMAS IN {catalog_source}\")\n",
    "    lst_schemas = [\n",
    "        t[0] for t in df_schemas.select(\"databaseName\").collect() if t[0] == schema_name\n",
    "    ]\n",
    "    if len(lst_schemas) > 0:\n",
    "        print(f\"Creating schema {schema_name} on catalog {catalog_target}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_target}.{schema_name}\")\n",
    "    else:\n",
    "        print(f\"Schema {schema_name} does not exist in {catalog_source}.\")\n",
    "\n",
    "\n",
    "catalog = \"spark_catalog\"\n",
    "\n",
    "schema=\"employeeDB\"\n",
    "# create_init_catalog(catalog)\n",
    "create_init_schema(catalog, schema)\n",
    "\n",
    "\n",
    "# Create a Delta table\n",
    "# data = spark.range(3, 8)\n",
    "# data.write.format(\"delta\").save(\"/tmp/delta-table3\")\n",
    "\n",
    "# Read data from the Delta table\n",
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table3\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Delta Lake Table\n",
    "\n",
    "-- SQL\n",
    "CREATE TABLE exampleDB.countries (\n",
    " id LONG,\n",
    " country STRING,\n",
    " capital STRING\n",
    ") USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "delta_table = (\n",
    "    DeltaTable\n",
    "    #.create(spark)\n",
    "    .createIfNotExists(spark)\n",
    "    .tableName(\"employeeDB.countries\")\n",
    "    .addColumn(\"id\", dataType=LongType(), nullable=False)\n",
    "    .addColumn(\"country\", dataType=StringType(), nullable=False)\n",
    "    .addColumn(\"capital\", dataType=StringType(), nullable=False)\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT INTO\n",
    "\n",
    "-- SQL\n",
    "INSERT INTO countries VALUES\n",
    "(1, 'United Kingdom', 'London'),\n",
    "(2, 'Canada', 'Toronto')\n",
    "With PySpark DataFrame syntax, you just need to specify that inserting records into\n",
    "a  specific  table  is  the  destination  of  a  write  operation  with  insertInto  (note  that\n",
    "columns are aligned positionally, so column names will be ignored with this method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"United Kingdom\", \"London\"),\n",
    "    (2, \"Canada\", \"Toronto\")\n",
    "    ]\n",
    "schema = [\"id\", \"country\", \"capital\"]\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "(\n",
    "df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".insertInto(\"countries\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(3, \"United States\", \"Washington, D.C.\")]\n",
    "# Define the schema for the Delta table\n",
    "schema = [\"id\", \"country\", \"capital\"]\n",
    "\n",
    "# Create a DataFrame from the sample data and schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "# Write the DataFrame to a Delta table in append mode\n",
    "# (if the table doesn't exist, it will be created)\n",
    "(df.write.format(\"delta\").mode(\"append\").saveAsTable(\"countries\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE TABLE AS SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Data\n",
    "\n",
    "a  high-level\n",
    "understanding of how partition filtering works (which is explored much more deeply\n",
    "in Chapters 5 and 10) and how the transaction log allows querying views of the data\n",
    "from previous versions with time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"countries\")\n",
    "df=delta_table.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Query\n",
    "-- SQL\n",
    "SELECT * FROM exampleDB.countries\n",
    "WHERE capital = \"London\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df=df.filter(df.capital == \"London\")\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- SQL\n",
    "SELECT\n",
    "    id,\n",
    "    capital\n",
    "FROM\n",
    "    countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df=df.select(\"id\", \"capital\")\n",
    "select_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- SQL\n",
    "SELECT DISTINCT id FROM countries VERSION AS OF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df = (\n",
    "    spark.read.option(\"versionAsOf\", \"1\")\n",
    "    .load(\"countries\")\n",
    "    .select(\"id\")\n",
    "    .distinct()\n",
    ")\n",
    "read_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- SQL\n",
    "#SELECT count(1) FROM exampleDB.countries TIMESTAMP AS OF \"2024-04-20\"\n",
    "# Python\n",
    "(\n",
    "spark\n",
    ".read\n",
    ".option(\"timestampAsOf\", \"2024-04-20\")\n",
    ".load(\"countries.delta\")\n",
    ".count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update\n",
    "\n",
    "-- SQL\n",
    "UPDATE countries\n",
    "SET { country = 'U.K.' }\n",
    "WHERE id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table.update(condition=\"id = 1\", set={\"country\": \"'U.K'\"})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete\n",
    "\n",
    "-- SQL\n",
    "DELETE FROM countries\n",
    "WHERE id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "delta_table.delete(\"id = 1\")  # uses SQL expression\n",
    "delta_table.delete(col(\"id\") == 2)  # uses PySpark expression\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.createDataFrame(\n",
    "        [(1, \"India\", \"New Delhi\"), (4, \"Australia\", \"Canberra\")],\n",
    "        schema=[\"id\", \"country\", \"capital\"],\n",
    "    )\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"overwrite\")  # specify the output mode\n",
    "    .saveAsTable(\"countries\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge\n",
    "\n",
    "-- SQL\n",
    "MERGE INTO countries A\n",
    "USING (select * from parquet.`countries.parquet`) B\n",
    "ON A.id = B.id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    id = A.id,\n",
    "    country = B.country,\n",
    "    capital = B.capital\n",
    "WHEN NOT MATCHED\n",
    "  THEN INSERT (\n",
    "    id,\n",
    "    country,\n",
    "    capital\n",
    "  )\n",
    "  VALUES (\n",
    "    B.id,\n",
    "    B.country,\n",
    "    B.capital\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = (\n",
    "    spark\n",
    "    .createDataFrame([\n",
    "        (1, 'India', 'New Delhi'),\n",
    "        (4, 'Australia', 'Canberra')],\n",
    "        schema=[\"id\", \"country\", \"capital\"]\n",
    "        )\n",
    "    )\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source=idf.alias(\"source\"), condition=\"source.id = target.id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\"country\": \"source.country\", \"capital\": \"source.capital\"}\n",
    ").whenNotMatchedInsert(\n",
    "    values={\"id\": \"source.id\", \"country\": \"source.country\", \"capital\": \"source.capital\"}\n",
    ").execute()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion\n",
    "\n",
    "-- SQL\n",
    "CONVERT TO DELTA parquet.`countries.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "'''delta_table = (\n",
    "    DeltaTable\n",
    "    .convertToDelta(\n",
    "        spark, \n",
    "        \"parquet.`countries.parquet`\"\n",
    "        )\n",
    "    )'''\n",
    "delta_table.detail()\n",
    "delta_table.history()\n",
    "# df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "name": "quickstart",
  "notebookId": 1927513300154480
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
