{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Delta Lake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/08 18:37:58 WARN Utils: Your hostname, Steves-Mac-mini.local resolves to a loopback address: 127.0.0.1; using 10.0.0.21 instead (on interface en1)\n",
      "24/12/08 18:37:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/slin/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/slin/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-39c5a79c-f5c4-4ab8-bc55-4a092f73ebea;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: resolution report :: resolve 90ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-39c5a79c-f5c4-4ab8-bc55-4a092f73ebea\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "24/12/08 18:37:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/08 18:38:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  7|\n",
      "|  5|\n",
      "|  4|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Create a Delta table\n",
    "#data = spark.range(3, 8)\n",
    "#data.write.format(\"delta\").save(\"/tmp/delta-table3\")\n",
    "\n",
    "# Read data from the Delta table\n",
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table3\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Delta Lake Table\n",
    "\n",
    "-- SQL\n",
    "CREATE TABLE exampleDB.countries (\n",
    " id LONG,\n",
    " country STRING,\n",
    " capital STRING\n",
    ") USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "delta_table = (\n",
    "    DeltaTable\n",
    "    #.create(spark)\n",
    "    .createIfNotExists(spark)\n",
    "    .tableName(\"countries\")\n",
    "    .addColumn(\"id\", dataType=LongType(), nullable=False)\n",
    "    .addColumn(\"country\", dataType=StringType(), nullable=False)\n",
    "    .addColumn(\"capital\", dataType=StringType(), nullable=False)\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT INTO\n",
    "\n",
    "-- SQL\n",
    "INSERT INTO countries VALUES\n",
    "(1, 'United Kingdom', 'London'),\n",
    "(2, 'Canada', 'Toronto')\n",
    "With PySpark DataFrame syntax, you just need to specify that inserting records into\n",
    "a  specific  table  is  the  destination  of  a  write  operation  with  insertInto  (note  that\n",
    "columns are aligned positionally, so column names will be ignored with this method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"United Kingdom\", \"London\"),\n",
    "    (2, \"Canada\", \"Toronto\")\n",
    "    ]\n",
    "schema = [\"id\", \"country\", \"capital\"]\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "(\n",
    "df\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".insertInto(\"countries\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(3, \"United States\", \"Washington, D.C.\")]\n",
    "# Define the schema for the Delta table\n",
    "schema = [\"id\", \"country\", \"capital\"]\n",
    "\n",
    "# Create a DataFrame from the sample data and schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "# Write the DataFrame to a Delta table in append mode\n",
    "# (if the table doesn't exist, it will be created)\n",
    "(df.write.format(\"delta\").mode(\"append\").saveAsTable(\"countries\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE TABLE AS SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Data\n",
    "\n",
    "a  high-level\n",
    "understanding of how partition filtering works (which is explored much more deeply\n",
    "in Chapters 5 and 10) and how the transaction log allows querying views of the data\n",
    "from previous versions with time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+----------------+\n",
      "| id|       country|         capital|\n",
      "+---+--------------+----------------+\n",
      "|  3| United States|Washington, D.C.|\n",
      "|  1|United Kingdom|          London|\n",
      "|  2|        Canada|         Toronto|\n",
      "+---+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"countries\")\n",
    "df=delta_table.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Query\n",
    "-- SQL\n",
    "SELECT * FROM exampleDB.countries\n",
    "WHERE capital = \"London\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------+\n",
      "| id|       country|capital|\n",
      "+---+--------------+-------+\n",
      "|  1|United Kingdom| London|\n",
      "+---+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df=df.filter(df.capital == \"London\")\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- SQL\n",
    "SELECT\n",
    "    id,\n",
    "    capital\n",
    "FROM\n",
    "    countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|         capital|\n",
      "+---+----------------+\n",
      "|  3|Washington, D.C.|\n",
      "|  1|          London|\n",
      "|  2|         Toronto|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_df=df.select(\"id\", \"capital\")\n",
    "select_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- SQL\n",
    "SELECT DISTINCT id FROM countries VERSION AS OF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Volumes/InvestAI/investai/investai-binder/notebook/countries.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m read_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mversionAsOf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcountries\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mdistinct()\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m read_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark_conda_env/lib/python3.13/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark_conda_env/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark_conda_env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Volumes/InvestAI/investai/investai-binder/notebook/countries."
     ]
    }
   ],
   "source": [
    "read_df = (\n",
    "    spark.read.option(\"versionAsOf\", \"1\")\n",
    "    .load(\"countries\")\n",
    "    .select(\"id\")\n",
    "    .distinct()\n",
    ")\n",
    "read_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- SQL\n",
    "#SELECT count(1) FROM exampleDB.countries TIMESTAMP AS OF \"2024-04-20\"\n",
    "# Python\n",
    "(\n",
    "spark\n",
    ".read\n",
    ".option(\"timestampAsOf\", \"2024-04-20\")\n",
    ".load(\"countries.delta\")\n",
    ".count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update\n",
    "\n",
    "-- SQL\n",
    "UPDATE countries\n",
    "SET { country = 'U.K.' }\n",
    "WHERE id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------------+\n",
      "| id|      country|         capital|\n",
      "+---+-------------+----------------+\n",
      "|  3|United States|Washington, D.C.|\n",
      "|  2|       Canada|         Toronto|\n",
      "|  1|          U.K|          London|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.update(condition=\"id = 1\", set={\"country\": \"'U.K'\"})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete\n",
    "\n",
    "-- SQL\n",
    "DELETE FROM countries\n",
    "WHERE id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------------+\n",
      "| id|      country|         capital|\n",
      "+---+-------------+----------------+\n",
      "|  3|United States|Washington, D.C.|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "delta_table.delete(\"id = 1\")  # uses SQL expression\n",
    "delta_table.delete(col(\"id\") == 2)  # uses PySpark expression\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "| id|  country|  capital|\n",
      "+---+---------+---------+\n",
      "|  4|Australia| Canberra|\n",
      "|  1|    India|New Delhi|\n",
      "+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.createDataFrame(\n",
    "        [(1, \"India\", \"New Delhi\"), (4, \"Australia\", \"Canberra\")],\n",
    "        schema=[\"id\", \"country\", \"capital\"],\n",
    "    )\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"overwrite\")  # specify the output mode\n",
    "    .saveAsTable(\"countries\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge\n",
    "\n",
    "-- SQL\n",
    "MERGE INTO countries A\n",
    "USING (select * from parquet.`countries.parquet`) B\n",
    "ON A.id = B.id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    id = A.id,\n",
    "    country = B.country,\n",
    "    capital = B.capital\n",
    "WHEN NOT MATCHED\n",
    "  THEN INSERT (\n",
    "    id,\n",
    "    country,\n",
    "    capital\n",
    "  )\n",
    "  VALUES (\n",
    "    B.id,\n",
    "    B.country,\n",
    "    B.capital\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "| id|  country|  capital|\n",
      "+---+---------+---------+\n",
      "|  1|    India|New Delhi|\n",
      "|  4|Australia| Canberra|\n",
      "+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = (\n",
    "    spark\n",
    "    .createDataFrame([\n",
    "        (1, 'India', 'New Delhi'),\n",
    "        (4, 'Australia', 'Canberra')],\n",
    "        schema=[\"id\", \"country\", \"capital\"]\n",
    "        )\n",
    "    )\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source=idf.alias(\"source\"), condition=\"source.id = target.id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\"country\": \"source.country\", \"capital\": \"source.capital\"}\n",
    ").whenNotMatchedInsert(\n",
    "    values={\"id\": \"source.id\", \"country\": \"source.country\", \"capital\": \"source.capital\"}\n",
    ").execute()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion\n",
    "\n",
    "-- SQL\n",
    "CONVERT TO DELTA parquet.`countries.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "'''delta_table = (\n",
    "    DeltaTable\n",
    "    .convertToDelta(\n",
    "        spark, \n",
    "        \"parquet.`countries.parquet`\"\n",
    "        )\n",
    "    )'''\n",
    "delta_table.detail()\n",
    "delta_table.history()\n",
    "# df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "name": "quickstart",
  "notebookId": 1927513300154480
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
